\subsection{Neural Networks}
Neural networks are a multi-layered collection of nodes which have an input layer, hidden layers, and an output layer \cite{AI-book}.
The input of each node is the output of all nodes in the previous layer.
Each of these connections in the network has an individual weight associated with it.
To get the value of a node not in the input layer, the output of all the nodes in the previous layer are added together with each individual weight along the connection that it travels.
All these weighted values are then fed to an aggregation function which result is given to an activation function.
The activation function is usually the Sigmoid, Sign, or Relu function.
\\
Neural networks can have any number of nodes in the input layer, hidden layers, and output layer and they do not need to be the same amount.
Furthermore, it can have any number of hidden layers.
\\
Neural networks have had considerable success in low-level reasoning where lots of training data are available.
They learn by giving the input layer values and then have the network compute an output value.
The output value is then compared to the real value of the inputs and based on the margin of error we go backwards through the network adjusting each weight.
This is called back propagation.
After doing this enough times or when the margin of error is lower than a predefined value the network is considered trained and can be tested on new data or be applied to a real-world scenario.

\subsubsection{Bayesian Personalized Ranking}
Bayesian Personalized Ranking(BPR) is an approach to modeling personalized ranking.
This is done through a pairwise approach where we calculate the probability that a user u prefers item i over item j.
It calculates the probability value of a user preferring i over j for every item pair and then maximizes the log-likelihood of all the observed pairs.
In NGCF, LightGCN and Price-Aware Recommendation they use BPR as their loss function.
This is done by taking the objective function of BPR $x_{uij}$ and negating it.
The equation for the loss function will then look like this:
\[E(x_{uij}) = ln(1+e^{-x_{uij}})\]
where the objective function is negated.

\subsubsection{Adam optimization algorithm}
