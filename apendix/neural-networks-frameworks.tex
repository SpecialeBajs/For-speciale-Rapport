\section{Python libraries for implementing neural networks}
During this project, we examined two implementations of recommendation systems which were LightGCN and Price aware recommendation with graph convolutional networks(PUP). 
The implementations are different from each other because they use different libraries when implementing the neural networks.
LightGCN uses TensorFlow and PUP uses PyTorch.

\subsection{PyTorch}
PyTorch has an object-oriented approach to creating neural networks.
A lot of the implementation is already done for you and you for the most part just need to set up the boilerplate code.
You start by creating a class that inherits from a class in PyTorch called \textit{nn.module}.
In the constructor of the class, you initialize all of the parameters in the network as tensors from PyTorch.
Finally you have to implement the forward function of the network which is used to process the input through the network.
The backward function that computes the gradients is already implemented.
After you have called the network with a given input you then can call the \textit{backward} function on the result to backpropagate the error.
PyTorch also has implementations of different update rules which can be used through their package called \textit{optim} which can be used to update the weights of the neural network after you have backpropagated the error.

\begin{lstlisting}[language=Python]
	import torch
	import torch.nn.Parameter import Parameter

	class NeuralNet(torch.nn.module):
		def __init__(self):
			super(NeuralNet, self).__init__()

			#initializes a tensor of the size 1000 x 64 as a Parameter in the neural network
			self.weights = Parameter(torch.FloatTensor(1000, 64))
		
		def forward(self, input):
			return torch.spmm(input, weights)

\end{lstlisting}

\begin{lstlisting}[language=Python]
	import torch
	import torch.optim

	optimizer = optim.SGD(net.parameters(), lr=0.01) #neural net optimizer using SGD update rules
	neural_net = NeuralNet()
	
	...

	def train_one_loop(input):
		#set the gradient buffers to zero. Otherwise they accumulate after every loop
		optimizer.zero_grad() 

		output = net(input)

		loss = our_loss_function(output)

		#backpropogates the error
		loss.backward() 

		#updates the weights in the neural net
		opitimizer.step() 

\end{lstlisting}

\subsection{TensorFlow}
Implementing a neural network in TensorFlow is very different from implementing a neural network in PyTorch.
TensorFlow does not use an object-oriented approach so you have to define everything manually.
You start by defining every variable that will be present in the neural network.
This might be the size of the input layer, hidden layers and output layer.
If you don't have a value to insert for them you still need to define placeholders so that TensorFlow has an idea of the shape and type of them.
After defining all of the variables you need to define the cost of the neural networks and the optimizer which can be the back propagating algorithm.
Just like in PyTorch TensorFlow has some optimizers that are already implemented for you. 
After you have defined all of the variables in your neural network you can initialize all of them by calling \textit{$initialize_all_variables$}.
You can then start a training session by instantiating the Session class in TensorFlow.
To start training you the call run function on the session object with the initialized variables.
From here you can start training by calling the run function again with the input data, the cost and the optimizer.
Every time we pass some input through the neural network with a cost and an optimizer it will back propagate and update the weights in the neural network.

\begin{lstlisting}[language=Python]
	import tensorflow as tf

	input_size = 1032
	hidden_size = 500
	output_size = 10
	epochs = 5
	batch_size = 128
	learning_rate = 0.01
	seed = 128

	#define placeholders
	input = tf.placeholder(tf.float32, [None, input_size])
	output = tf.placeholder(tf.float32, [None, output_size])

	#define weights and biases of neural network
	weights = {
		'hidden': tf.Variable(tf.random_normal([input_size, hidden_size], seed=seed)),
		'output': tf.Variable(tf.random_normal([hidden_size, output_size], seed=seed))
	}

	biases = {
		'hidden': tf.Variable(tf.random_normal([hidden_size], seed=seed)),
    	'output': tf.Variable(tf.random_normal([output_size], seed=seed))
	}

	#create neural network computational graph
	hidden_layer = tf.add(tf.matmul(x, weights['hidden']), biases['hidden'])
	hidden_layer = tf.nn.relu(hidden_layer)

	output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']

	#define cost
	cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_layer, y))

	#define optimizer
	optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

	#after everything is set up we need to initialize all the variables
	init = tf.initialize_all_variables()

	#create a session and start learning
	with tf.Session() as session:

		#create the initialized variables in the current session
		session.run(init)

		#start training
		for epoch in range(epoch)
			avg_cost = 0

			#training data is divided into batches of size batch_size
			#here we get the number of batches we will go through
			num_of_batches = int(size_of_training_data / batch_size)
			
			for i in range(num_of_batches)
				#some function we have implemented that gets us one batch of training data
				batch = our_get_batch_function(i)

				#run the batch through the neural network
				_, c = session.run([optimizer, cost], feed_dict = {x: batch})

				avg_cost += c / total_batch
\end{lstlisting}
